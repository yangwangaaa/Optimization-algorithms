# Gradient Descent

Different variants on gradient descent.

### [Gradient Descent](https://github.com/WardQ/Optimization-algorithms/tree/master/Gradient%20Descent/Gradient%20Descent/)
Basic gradient descent algorithm.

![](https://github.com/WardQ/Optimization-algorithms/blob/master/Gradient%20Descent/Gradient%20Descent/result2.png)

### [Mini-Batch Adam](https://github.com/WardQ/Optimization-algorithms/tree/master/Gradient%20Descent/Mini-Batch%20Adam)

Adaptive Moment Estimation (ADAM) is a combination of RMSProp and Momentum. Often used in Deep Neural networks. Mini-batch processes only a part of the dataset, instead of the whole dataset.

### [Mini-Batch Gradient Descent with Momentum](https://github.com/WardQ/Optimization-algorithms/tree/master/Gradient%20Descent/Mini-Batch%20Gradient%20Descent%20with%20Momentum)

Gradient descent with momentum, which stabilizes the direction of the steps taken. Mini-batch processes only a part of the dataset, instead of the whole dataset.

### [Simulated Annealing](https://github.com/WardQ/Optimization-algorithms/tree/master/Gradient%20Descent/Simulated%20Annealing)

Simulated Annealing is an extention of gradient descent, where certain 'worse' solutions can be accepted temporarely. It succeeds in escaping local minima.

![](https://github.com/WardQ/Optimization-algorithms/blob/master/Gradient%20Descent/Simulated%20Annealing/alpine_simulated_annealing.png)
